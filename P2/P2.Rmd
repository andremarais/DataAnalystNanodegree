---
title: "P2: Analyzing the NYC Subway Data set"

---

### <span style="color:DodgerBlue;">Section 0.</span>
#### <span style="color:DodgerBlue;">References </span>
**http://stackoverflow.com** 
Mostly topics concerning pandas, numpy, and ggplot. </br>
**http://pandas.pydata.org**
Alternative site to stackoverflow for pandas and ggplot information. </br>
**http://ggplot.yhathq.com**
For ggplot syntax - ggplot is slightly different in Python as opposed to R. </br>




### <span style="color:DodgerBlue;">Section 1.</span>
#### <span style="color:DodgerBlue;">Statistical Test </span>
**<span style="color:DodgerBlue;"> 1.1</span>** I decided on doing the Mann-Whitney test. By looking at the graph from the previous exercise, you can see that one can expect higher ridership on rainy days. The null hypothesis is that there is no difference in ridership between rainy and non-rainy days. The alternative would be that the ridership are higher on rainy days. Therefor we can use the one-tail test. For this test we'll use the conventional alpha value of 5%. </br>
**<span style="color:DodgerBlue;"> 1.2</span>** The Mann-Whitney test is non-parametric, which means we don't have to make any assumptions regarding the underlying distribution of the ridership. The Mann-Whitney test uses rank sum statistics to make inference on shifts between data sets. By using ranks, you forgo any need to make assumptions on the data.</br> 
**<span style="color:DodgerBlue;"> 1.3</span>** The U statistic from the test by itself (1924409167) doesn't say much, but the corresponding p-value is 0.025 (rounded to 3 digits), which means that the probability of getting this U values is 2.5%. The mean ridership means are 1105.446, 1090.279 for rainy and non-rainy days respectively.  </br> 
**<span style="color:DodgerBlue;"> 1.3</span>** With a p-value of less than 5%, we can safely reject the null hypothesis in favor of the alternative. In other words, there is a significant increase in ridership on rainy days, as opposed to non-rainy days.</br> 

### <span style="color:DodgerBlue;">Section 2.</span>
#### <span style="color:DodgerBlue;">Linear Regression </span>
**<span style="color:DodgerBlue;"> 2.1</span>** For linear regression I used the order of least squares method from the Stats models library. </br>
**<span style="color:DodgerBlue;"> 2.2 & 2.3</span>** Here's a list of the features I used, with the reasoning behind it: </br>

<ul>
<li>Rain: Ridership will pick up on rainy days, since commuters might rather take the train instead of making use of the buses or their own vehicles for safety reasons. </li>
<li>Mean wind speed: No one likes standing outside on a windy day.</li>
<li>Hour of the day: Hour to pick up on the daily ridership pattern
<li>Fog indicator: Reduced visibility might persuade commuters to rather make use of the subway instead of using their own vehicles</li></li>
<li>Maximum temperature for the day: On hot or cold days, people might prefer making use of a transport method which provide air conditioning </li>
<li>Mean temperature for the day: Same as above. I found that you get a better $R^2$ value when you include both the mean and maximum temperature. By also including the minimum, you actually lose accuracy </li>
<li>With regards to the dummy variables, I used the turnstiles' unit numbers and the dates as dummy variables. Using the days drastically improves the accuracy of the model.</li>
</ul>

**<span style="color:DodgerBlue;"> 2.4</span>** List of the non-dummy features' coefficients, all rounded down to 3 digits.

Feature | coefficient
--- | --- | ---
rain | 56.014
meanwindspdi | 51.767
Hour | 65.3
fog | 139.522
maxtempi | 25.892
meantempi | -23.807

**<span style="color:DodgerBlue;"> 2.5</span>** The $R^2$ value for this model is 0.493.

**<span style="color:DodgerBlue;"> 2.6</span>** To judge a model's accuracy can be subjective, especially if the $R^2$ value is not too close to either 0 or 1. An $R^2$ value too close 1 means you over fit your model, which is not a very good idea. Having played around with the features, I think this is a relatively good $R^2$ value, but other modelling techniques might prove to be more accurate. Linear models aren't always the best models to use to predict your dependent variables. I've found that a biased sample can through your model completely off. It might be a good idea to bootstrap your data, and ensemble a few linear models. I'm a big fan of Random Forest models, but they can become monsters if you have too many variables. In this case we have an additional 495 features (465 Unit dummy variables and 30 date variables), so a Random Forest might not be the nicest solution. 
